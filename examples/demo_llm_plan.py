#!/usr/bin/env python
"""
demo_llm_plan.py
================

Natural-language task â†’ LLM-generated JSON plan â†’ executed plan
"""

from __future__ import annotations
import argparse, asyncio, json, os, re
from typing import Dict, Any, List

# â”€â”€ demo tools (register on import) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from sample_tools import WeatherTool, CalculatorTool, SearchTool  # noqa: F401

# â”€â”€ A2A plumbing -----------------------------------------------------
from a2a_session_manager.storage import InMemorySessionStore, SessionStoreProvider
from a2a_session_manager.models.session import Session
from chuk_ai_planner.store.memory import InMemoryGraphStore
from chuk_ai_planner.planner import Plan
from chuk_ai_planner.models import ToolCall
from chuk_ai_planner.models.edges import GraphEdge, EdgeKind
from chuk_ai_planner.processor import GraphAwareToolProcessor
from chuk_ai_planner.utils.visualization import print_session_events, print_graph_structure
from chuk_ai_planner.utils.registry_helpers import execute_tool

from dotenv import load_dotenv
load_dotenv()


# â”€â”€ OpenAI (optional) -------------------------------------------------
from chuk_ai_planner.demo.llm_simulator import simulate_llm_call
try:
    from openai import AsyncOpenAI
except ImportError:
    AsyncOpenAI = None  # type: ignore

# â”€â”€ quick registry name helper ---------------------------------------
from chuk_tool_processor.registry import default_registry
def registry_names():
    if hasattr(default_registry, "iter_tools"):
        for meta in default_registry.iter_tools():
            yield meta.name
    elif hasattr(default_registry, "tools"):
        yield from default_registry.tools.keys()             # type: ignore[attr-defined]
    else:
        yield from getattr(default_registry, "_tools", {}).keys()

# universal async adapter (same as earlier demo) ----------------------
async def adapter(tool_name: str, args: Dict[str, Any]) -> Any:
    tc = {"id": "call",
          "type": "function",
          "function": {"name": tool_name, "arguments": json.dumps(args)}}
    return await execute_tool(tc, _parent_event_id=None, _assistant_node_id=None)

def reg(proc: GraphAwareToolProcessor, name: str):
    proc.register_tool(name, lambda a, _n=name: adapter(_n, a))

# -------------------------------------------------------------------- LLM helpers
LLM_SYSTEM_MSG = (
    "You are an assistant that converts a natural-language task into a JSON "
    "plan. Return ONLY valid JSON!\n"
    "Schema:\n"
    "{\n"
    '  "title": str,\n'
    '  "steps": [              // ordered list\n'
    '    {"title": str, "depends_on": [indices]},\n'
    "    ...\n"
    "  ]\n"
    "}\n"
    "Indices start at 1 in the final output."
)


async def call_llm_live(task: str) -> Dict[str, Any]:
    if not AsyncOpenAI:
        raise RuntimeError("openai package not installed")
    client = AsyncOpenAI()
    resp = await client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.3,
        messages=[
            {"role": "system", "content": LLM_SYSTEM_MSG},
            {"role": "user", "content": task},
        ],
    )
    return json.loads(resp.choices[0].message.content)


async def call_llm_sim(task: str) -> Dict[str, Any]:
    """
    Crude rule-based stub that returns a fixed three-step plan for any prompt.
    """
    _ = await simulate_llm_call(task)  # just to show the prompt
    return {
        "title": "Weather â†’ Calc â†’ Search",
        "steps": [
            {"title": "Check weather in New York",           "depends_on": []},
            {"title": "Multiply 235.5 Ã— 18.75",              "depends_on": []},
            {"title": "Search climate-adaptation info",      "depends_on": []},
        ],
    }

# -------------------------------------------------------------------- main flow
async def main(live: bool) -> None:
    task = (
        "I need a short plan that first prepares coffee "
        "then checks today's weather in New York, multiplies 235.5Ã—18.75, "
        "and finally searches for pages on climate-change adaptation."
    )

    llm_json = await (call_llm_live if live else call_llm_sim)(task)

    # 1) build Plan DSL -----------------------------------------------
    plan = Plan(llm_json["title"])
    for i, step in enumerate(llm_json["steps"], 1):
        deps = [str(d) for d in step.get("depends_on", [])]
        plan.step(step["title"], after=deps).up()
    plan_id = plan.save()

    print("\nðŸ“‹  PLAN GENERATED BY LLM\n")
    print(plan.outline(), "\n")

    # 2) naive tool-mapping heuristic ---------------------------------
    # (real usage would rely on function-calling in the LLM)
    title_map = {
        re.compile(r"weather", re.I):   ("weather",    {"location": "New York"}),
        re.compile(r"multiply", re.I):  ("calculator", {"operation": "multiply",
                                                       "a": 235.5, "b": 18.75}),
        re.compile(r"search", re.I):    ("search",     {"query":
                                                        "climate change adaptation"}),
        re.compile(r"grind", re.I):     ("grind_beans",  {}),
        re.compile(r"boil", re.I):      ("boil_water",   {}),
        re.compile(r"brew", re.I):      ("brew_coffee",  {}),
        re.compile(r"clean", re.I):     ("clean_station",{}),
    }

    idx2id = {n.data["index"]: n.id
              for n in plan.graph.nodes.values()
              if n.__class__.__name__ == "PlanStep"}

    for idx, node_id in idx2id.items():
        title = next(n.data["description"]
                     for n in plan.graph.nodes.values()
                     if n.id == node_id)
        for pattern, (tool, args) in title_map.items():
            if pattern.search(title):
                tc = ToolCall(data={"name": tool, "args": args})
                plan.graph.add_node(tc)
                plan.graph.add_edge(GraphEdge(kind=EdgeKind.PLAN_LINK,
                                              src=node_id, dst=tc.id))
                break

    # 3) processor ----------------------------------------------------
    SessionStoreProvider.set_store(InMemorySessionStore())
    session = Session(); SessionStoreProvider.get_store().save(session)

    proc = GraphAwareToolProcessor(session_id=session.id, graph_store=plan.graph)

    # registry & demo tools
    for n in registry_names():
        reg(proc, n)
    reg(proc, "weather")
    reg(proc, "calculator")
    reg(proc, "search")
    proc.register_tool("grind_beans",   adapter.__get__(None, object))
    proc.register_tool("boil_water",    adapter.__get__(None, object))
    proc.register_tool("brew_coffee",   adapter.__get__(None, object))
    proc.register_tool("clean_station", adapter.__get__(None, object))

    # run plan
    results = await proc.process_plan(
        plan_node_id=plan_id,
        assistant_node_id="assistant",
        llm_call_fn=lambda _: None,
    )

    print("âœ…  TOOL RESULTS\n")
    for r in results:
        print(f"â€¢ {r.tool}\n{json.dumps(r.result, indent=2)}\n")

    print_session_events(session)
    print_graph_structure(plan.graph)

# -------------------------------------------------------------------- entry-point
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--live", action="store_true",
                        help="Use real OpenAI rather than the simulator")
    args = parser.parse_args()

    if args.live and not os.getenv("OPENAI_API_KEY"):
        parser.error("Set OPENAI_API_KEY or omit --live")

    asyncio.run(main(args.live))
